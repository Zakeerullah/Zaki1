{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1NysEWZ9ihksGQBNQ1lDAQCL4mrGJHwHN",
      "authorship_tag": "ABX9TyPxd5A75e3v1tq3OE7UGhcH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zakeerullah/Zaki1/blob/main/Prediction_on_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2  # for video processing\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "I-BnPbJe4CM2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class YourModelClass(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(YourModelClass, self).__init__()\n",
        "        # Load pretrained AlexNet\n",
        "        alexnet = models.alexnet(pretrained=True)\n",
        "\n",
        "        # Replace the classifier with a new one (the one you've defined)\n",
        "        self.features = alexnet.features\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(9216, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),  # Add dropout layer\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),  # Add another dropout layer\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)  # Pass the input through the feature extractor\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output for the classifier\n",
        "        x = self.classifier(x)  # Pass the flattened output through the classifier\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "NwSwI8xX5E0I"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load the trained model\n",
        "model = YourModelClass(num_classes=2)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/Model/Alexnet_model.pth'))\n",
        "model.eval()  # Set the model to evaluation mode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sdkjs8z75JW3",
        "outputId": "c540f07a-130d-495d-f6df-5bac6836ca5c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "YourModelClass(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU()\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RiZfi8rmZdM6"
      },
      "outputs": [],
      "source": [
        "# Define the transformation steps\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.ToPILImage(),  # Convert a tensor or an ndarray to PIL Image\n",
        "    transforms.Resize(256),  # Resize the image to 256x256 pixels\n",
        "    transforms.CenterCrop(224),  # Crop the image to 224x224 pixels about the center\n",
        "    transforms.ToTensor(),  # Convert the image to a tensor with pixels in the range [0, 1]\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize with ImageNet's mean and std\n",
        "])\n",
        "\n",
        "# Step 2: Preprocess video frames (assuming you have a function for this)\n",
        "def preprocess_frame(frame):\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert frame from BGR to RGB\n",
        "    frame = preprocess(frame)  # Apply the preprocessing steps\n",
        "    return frame.unsqueeze(0)\n",
        "\n",
        "# Step 3: Predict each frame\n",
        "def predict_video(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    predictions = []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        processed_frame = preprocess_frame(frame)\n",
        "        with torch.no_grad():  # Ensure no gradients are calculated\n",
        "            outputs = model(processed_frame)\n",
        "            probabilities = F.softmax(outputs, dim=1)  # Apply softmax to convert logits to probabilities\n",
        "            _, predicted = torch.max(probabilities, 1)  # Get the predicted class index\n",
        "            predictions.append(predicted.item())\n",
        "\n",
        "    cap.release()\n",
        "    return predictions\n",
        "\n",
        "\n",
        "# Step 4: Majority voting to determine video label\n",
        "def get_video_label(predictions):\n",
        "    # Assuming 1 for real and 0 for fake\n",
        "    threshold = len(predictions) / 2\n",
        "    return 'real' if sum(predictions) > threshold else 'fake'\n",
        "\n",
        "\n",
        "\n",
        "# Step 5: Tagging video with label\n",
        "def tag_video(video_path, label):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    out = None\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        if out is None:\n",
        "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "            out = cv2.VideoWriter('tagged_video_01.mp4', fourcc, cap.get(cv2.CAP_PROP_FPS),\n",
        "                                  (frame.shape[1], frame.shape[0]))\n",
        "\n",
        "\n",
        "        text = label\n",
        "        font_scale = 4\n",
        "        thickness = 8\n",
        "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "\n",
        "        # Get the width and height of the text box\n",
        "        (text_width, text_height), baseline = cv2.getTextSize(text, font, font_scale, thickness)\n",
        "\n",
        "        # Calculate the starting x and y coordinates to center the text\n",
        "        startX = (frame.shape[1] - text_width) // 2\n",
        "        startY = (frame.shape[0] + text_height) // 2\n",
        "        # Add label to frame\n",
        "        cv2.putText(frame, text, (startX, startY), font,\n",
        "            font_scale, (0, 255, 0), thickness, cv2.LINE_AA)\n",
        "\n",
        "        out.write(frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "video_path = '/content/drive/MyDrive/Colab Notebooks/0test1.mp4'\n",
        "predictions = predict_video(video_path)\n",
        "video_label = get_video_label(predictions)\n",
        "tag_video(video_path, video_label)\n",
        "\n",
        "# Save model\n",
        "#torch.save(model.state_dict(), 'Prediction_model.pth')\n",
        "#model_save_path = '/content/drive/MyDrive/Colab Notebooks/Model/Prediction_model.pth'\n",
        "#torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using both vgg16 and alexnet"
      ],
      "metadata": {
        "id": "NHeDl6BqxDTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "##(import torch\n",
        "import cv2  # for video processing\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "# Define your AlexNet-based model class\n",
        "class YourAlexNetModelClass(nn.Module):\n",
        "    # ... (same as before)\n",
        "\n",
        "# Define your VGG16-based model class\n",
        "class YourVGG16ModelClass(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(YourVGG16ModelClass, self).__init__()\n",
        "        # Load pretrained VGG16\n",
        "        vgg16 = models.vgg16(pretrained=True)\n",
        "\n",
        "        # Replace the classifier with a new one (the one you've defined)\n",
        "        self.features = vgg16.features\n",
        "        self.classifier = nn.Sequential(\n",
        "            # ... (define your classifier layers here)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)  # Pass the input through the feature extractor\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output for the classifier\n",
        "        x = self.classifier(x)  # Pass the flattened output through the classifier\n",
        "        return x\n",
        "\n",
        "# Load both trained models\n",
        "alexnet_model = YourAlexNetModelClass(num_classes=2)\n",
        "alexnet_model.load_state_dict(torch.load('Alexnet_model.pth'))\n",
        "alexnet_model.eval()  # Set to evaluation mode\n",
        "\n",
        "vgg16_model = YourVGG16ModelClass(num_classes=2)\n",
        "vgg16_model.load_state_dict(torch.load('VGG16_model.pth'))\n",
        "vgg16_model.eval()  # Set to evaluation mode\n",
        "\n",
        "# Define preprocessing steps for each model if they differ\n",
        "preprocess_alexnet = transforms.Compose([\n",
        "    # ... (same as before for AlexNet)\n",
        "])\n",
        "\n",
        "preprocess_vgg16 = transforms.Compose([\n",
        "    # ... (define preprocessing steps for VGG16 if different)\n",
        "])\n",
        "\n",
        "# Modify preprocess_frame function to handle both models\n",
        "def preprocess_frame(frame, model_name):\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert frame from BGR to RGB\n",
        "\n",
        "    if model_name == 'alexnet':\n",
        "        frame = preprocess_alexnet(frame)  # Apply AlexNet preprocessing steps\n",
        "    elif model_name == 'vgg16':\n",
        "        frame = preprocess_vgg16(frame)  # Apply VGG16 preprocessing steps\n",
        "\n",
        "    return frame.unsqueeze(0)\n",
        "\n",
        "# Modify predict_video function to get predictions from both models\n",
        "def predict_video(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    alexnet_predictions = []\n",
        "    vgg16_predictions = []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        processed_frame_alexnet = preprocess_frame(frame, 'alexnet')\n",
        "        processed_frame_vgg16 = preprocess_frame(frame, 'vgg16')\n",
        "\n",
        "        with torch.no_grad():  # Ensure no gradients are calculated\n",
        "            alexnet_prediction = alexnet_model(processed_frame_alexnet)\n",
        "            vgg16_prediction = vgg16_model(processed_frame_vgg16)\n",
        "\n",
        "            alexnet_predictions.append(alexnet_prediction.item())\n",
        "            vgg16_predictions.append(vgg16_prediction.item())\n",
        "\n",
        "    cap.release()\n",
        "    return alexnet_predictions, vgg16_predictions\n",
        "\n",
        "# Modify get_video_label function to handle predictions from both models\n",
        "def get_video_label(alexnet_predictions, vgg16_predictions):\n",
        "    combined_predictions = []\n",
        "\n",
        "    for alex_pred, vgg_pred in zip(alexnet_predictions, vgg16_predictions):\n",
        "        combined_pred = (alex_pred + vgg_pred) / 2  # Average predictions from both models\n",
        "        combined_predictions.append(combined_pred)\n",
        "\n",
        "    threshold = len(combined_predictions) / 2\n",
        "    return 'real' if sum(combined_predictions) > threshold else 'fake'\n",
        "\n",
        "# The rest of your code remains unchanged\n",
        "\n",
        "# Example usage:\n",
        "video_path = 'path_to_your_video.mp4'\n",
        "alexnet_preds, vgg16_preds = predict_video(video_path)\n",
        "video_label = get_video_label(alexnet_preds, vgg16_preds)\n",
        "tag_video(video_path, video_label)\n",
        "\n",
        "###this code is incomplete###\n"
      ],
      "metadata": {
        "id": "emwgD10bxGrq",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}